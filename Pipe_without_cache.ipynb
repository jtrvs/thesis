{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdd301c",
   "metadata": {},
   "source": [
    "# Пайплайн с полным пересчётом и записью только малой матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32385f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"cef83d0f-6307-4904-b99d-0c088cf98a20\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"cef83d0f-6307-4904-b99d-0c088cf98a20\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"cef83d0f-6307-4904-b99d-0c088cf98a20\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"cef83d0f-6307-4904-b99d-0c088cf98a20\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"cef83d0f-6307-4904-b99d-0c088cf98a20\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58c184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 3.5.5\n",
      "SparkUI available at http://192.168.0.112:4041\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.134-952ae203dbbe\n",
      "LOGGING: writing to /home/julia/Documents/thesis/hail-20250429-0952-0.2.134-952ae203dbbe.log\n",
      "2025-04-29 10:19:46.776 Hail: WARN: '/home/julia/Documents/thesis/cf_for_generate/extended_sex_info.csv' refers to no files\n",
      "2025-04-29 10:20:19.896 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'ID' as type str (user-supplied)\n",
      "  Loading field 'sex' as type str (user-supplied)\n",
      "  Loading field 'profile' as type str (not specified)\n",
      "2025-04-29 10:21:01.948 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'ID' as type str (user-supplied)\n",
      "  Loading field 'sex' as type str (user-supplied)\n",
      "  Loading field 'profile' as type str (not specified)\n",
      "2025-04-29 10:25:06.513 Hail: INFO: scanning VCF for sortedness...\n",
      "2025-04-29 10:25:23.740 Hail: INFO: Coerced sorted VCF - no additional import work to do\n",
      "2025-04-29 10:25:23.786 Hail: INFO: scanning VCF for sortedness...\n",
      "2025-04-29 10:25:39.743 Hail: INFO: Coerced sorted VCF - no additional import work to do\n",
      "2025-04-29 10:25:39.804 Hail: INFO: scanning VCF for sortedness...\n",
      "2025-04-29 10:25:55.411 Hail: INFO: Coerced sorted VCF - no additional import work to do\n",
      "2025-04-29 10:25:55.463 Hail: INFO: scanning VCF for sortedness...\n",
      "2025-04-29 10:26:10.941 Hail: INFO: Coerced sorted VCF - no additional import work to do\n",
      "2025-04-29 10:26:10.978 Hail: INFO: scanning VCF for sortedness...\n",
      "2025-04-29 10:26:24.189 Hail: INFO: Coerced sorted VCF - no additional import work to do\n",
      "2025-04-29 10:26:24.240 Hail: INFO: scanning VCF for sortedness...\n"
     ]
    }
   ],
   "source": [
    "hl.init()\n",
    "hl.default_reference('GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d6c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a296037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# конфигурация\n",
    "# VCF_DIR = '/home/julia/Downloads/gnomADru/vcf/'  # папка с VCF\n",
    "# VCF_DIR = '/home/julia/Downloads/gnomADru/vcf_test/'  # папка с VCF без одного варианта\n",
    "VCF_DIR = '/home/julia/Documents/thesis/vcf_for_generate/'  # папка сVCF\n",
    "\n",
    "VCF_DIR_NEW = '/home/julia/Documents/thesis/vcf_new/' # папка с VCF для добавления\n",
    "# SEX_TABLE_PATH = '/home/julia/Downloads/gnomADru/sids.csv' # файл с полом\n",
    "# SEX_TABLE_PATH = '/home/julia/Downloads/gnomADru/vcf_test/sids_test.csv' # файл с полом - тестовый, с одним \"лишним\" образцом\n",
    "SEX_TABLE_PATH = '/home/julia/Documents/thesis/vcf_for_generate/extended_sex_info.csv' # пол нагенерированных файлов\n",
    "\n",
    "AF_PATH = '/home/julia/Documents/thesis/cache/af.mt' # частоты отдельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a627524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/julia/Documents/thesis/vcf_for_generate/000007000020.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000040.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000070.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000030.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000060.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000080.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000081.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000082.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000083.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000084.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000085.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000086.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000087.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000088.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000089.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000090.vcf.gz']\n"
     ]
    }
   ],
   "source": [
    "# старые файлы\n",
    "\n",
    "vcf_files = glob.glob(VCF_DIR + '*.vcf.gz')\n",
    "print(vcf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0f9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/julia/Documents/thesis/vcf_new/000007000050.vcf.gz']\n"
     ]
    }
   ],
   "source": [
    "# новые файлы\n",
    "new_vcf_files = glob.glob(VCF_DIR_NEW + '*.vcf.gz')\n",
    "print(new_vcf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503e1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавление пола\n",
    "\n",
    "def set_sex(mt, sex_table):\n",
    "    # преобразуем пол в is_female (True для 'ж'/'f')\n",
    "    sex_table = sex_table.annotate(\n",
    "        is_female = (\n",
    "            (sex_table.sex.lower() == 'ж') | \n",
    "            (sex_table.sex.lower() == 'f')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # добавляем is_female к образцам (простое соединение)\n",
    "    mt = mt.annotate_cols(\n",
    "        is_female = sex_table[mt.s].is_female  # mt.s - ID образца\n",
    "    )\n",
    "\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a73f99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормализация гемизигот у мужчин\n",
    "\n",
    "def gemizygote_normalize(mt):\n",
    "    return mt.annotate_entries(\n",
    "        GT = hl.if_else(\n",
    "            (~mt.is_female) & ((mt.locus.contig == \"chrX\") | (mt.locus.contig == \"chrY\")),\n",
    "            hl.if_else(\n",
    "                mt.GT.is_hom_ref(),  # гомозигота по REF 0/0 → 0\n",
    "                hl.call(0),\n",
    "                hl.if_else(\n",
    "                    mt.GT.is_hom_var(),  # гомозигота по ALT 1/1 → 1\n",
    "                    hl.call(1),\n",
    "                    hl.if_else(\n",
    "                        mt.VAF[0] > 0.3,  # для гетерозигот - если VAF > 30% → 1 считаем гомозиготой по ALT\n",
    "                        hl.call(1),\n",
    "                        hl.call(0)     # иначе → 0 считаем гомозиготой по REF\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            mt.GT  # Для женщин и аутосом оставляем без изменений\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "504d612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# фильтрация по глубине\n",
    "\n",
    "def filter_variants_by_DP(combined_mt_all, dp):\n",
    "\n",
    "    # отсекаем варианты, если нет ни одного образца с DP больше порога\n",
    "    filtered_mt = combined_mt_all.filter_rows(\n",
    "        hl.agg.count_where(\n",
    "            (hl.is_defined(combined_mt_all.DP)) & \n",
    "            (combined_mt_all.DP >= dp)\n",
    "        ) >= 1\n",
    "    )\n",
    "\n",
    "    # корректируем генотипы - варианты с DP меньше порога исключаем из расчёта частот, помечая как NA\n",
    "    return filtered_mt.annotate_entries(\n",
    "        GT = hl.if_else(\n",
    "            (hl.is_defined(filtered_mt.DP)) & \n",
    "            (filtered_mt.DP >= dp),\n",
    "            filtered_mt.GT,\n",
    "            hl.missing(hl.tcall)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f5d6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#препроцессинг до фильтрации включительно\n",
    "\n",
    "def preprocessing(vcf_files, sex_table):\n",
    "    # комбайн\n",
    "    mts_all = []\n",
    "    for vcf in vcf_files: \n",
    "        mt = hl.import_vcf(vcf, force_bgz=True, array_elements_required=False)\n",
    "        mt = set_sex(mt, sex_table)\n",
    "        mt = gemizygote_normalize(mt)    \n",
    "        mts_all.append(mt)\n",
    "\n",
    "    # Объединение MatrixTable по колонкам (образцам)\n",
    "    combined_mt_all = mts_all[0]\n",
    "    if len(mts_all) > 1:\n",
    "        for mt in mts_all[1:]:\n",
    "            combined_mt_all = combined_mt_all.union_cols(mt, row_join_type='outer')\n",
    "\n",
    "    #фильтрация по глубине\n",
    "    return filter_variants_by_DP(combined_mt_all, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91cd7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# расчёт частот - 4 версия.попытаться ускорить.пока ориентировочно лучший вариант\n",
    "\n",
    "def mt_AF_calculated(mt):\n",
    "    freq_mt_all = mt.annotate_rows(\n",
    "    call_stats=hl.agg.call_stats(mt.GT, mt.alleles)\n",
    "    )\n",
    "\n",
    "    # извлечение частот аллелей\n",
    "    return freq_mt_all.annotate_rows(\n",
    "        allele_frequencies=freq_mt_all.call_stats.AF  # AF — это массив частот аллелей, включая мультиаллели\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d0731",
   "metadata": {},
   "source": [
    "## Пайплайн с добавлением новых данных - отсюда и до конца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb90393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определение пола\n",
    "sex_table = hl.import_table(SEX_TABLE_PATH,\n",
    "        delimiter=',',\n",
    "        types={'ID': hl.tstr, 'sex': hl.tstr},\n",
    "        key='ID'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38dcd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/julia/Documents/thesis/vcf_new/000007000050.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000020.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000040.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000070.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000030.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000007000060.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000080.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000081.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000082.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000083.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000084.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000085.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000086.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000087.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000088.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000089.vcf.gz', '/home/julia/Documents/thesis/vcf_for_generate/000070000090.vcf.gz']\n"
     ]
    }
   ],
   "source": [
    "# vcf_files = new_vcf_files + vcf_files\n",
    "# print(vcf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "604b4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 ms, sys: 14.8 ms, total: 789 ms\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# препроцессинг данных\n",
    "data = preprocessing(vcf_files, sex_table)\n",
    "\n",
    "#16 образцов\n",
    "#CPU times: user 774 ms, sys: 14.8 ms, total: 789 ms\n",
    "#Wall time: 1.74 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54cd7590-6a43-4cce-b056-f609d21f4b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 2.73 ms, total: 23.9 ms\n",
      "Wall time: 21.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# расчёт частот\n",
    "data_af = mt_AF_calculated(data)\n",
    "\n",
    "#16 образцов\n",
    "#CPU times: user 21.2 ms, sys: 2.73 ms, total: 23.9 ms\n",
    "#Wall time: 21.6 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fdef55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'is_female': bool\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'qual': float64\n",
      "    'filters': set<str>\n",
      "    'info': struct {\n",
      "        END: int32\n",
      "    }\n",
      "    'call_stats': struct {\n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        AN: int32, \n",
      "        homozygote_count: array<int32>\n",
      "    }\n",
      "    'allele_frequencies': array<float64>\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "    'GQ': int32\n",
      "    'DP': int32\n",
      "    'MIN_DP': int32\n",
      "    'AD': array<int32>\n",
      "    'VAF': array<float64>\n",
      "    'PL': array<int32>\n",
      "    'MED_DP': int32\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_af.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41eb31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/julia/anaconda3/envs/hail/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.5.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 10:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10918244, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_af.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6f1ff40-f6f4-4b7a-b1fd-8262660ef48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_af_table(data_af):\n",
    "    filtered_mt = data_af.select_entries('DP', 'AD') \\\n",
    "                   .select_rows('rsid', 'allele_frequencies')\n",
    "    return filtered_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "357c1c6d-5b63-4c0a-b833-b17278c62ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 ms, sys: 142 μs, total: 16.3 ms\n",
      "Wall time: 15.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "af = make_af_table(data_af)\n",
    "\n",
    "# 16 образцов\n",
    "# CPU times: user 16.1 ms, sys: 142 μs, total: 16.3 ms\n",
    "# Wall time: 15.9 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "850d4d54-598d-44e9-8548-4d7a4cd9a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/apache-spark/jars/spark-core_2.12-3.5.5.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 25:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25) (192.168.0.112 executor driver): is.hail.utils.HailException: file:/home/julia/Documents/thesis/vcf_for_generate/000070000080.vcf.gz:offset 5026524526671: error while parsing line\nHPV61\t5833\t.\tA\tC\t88.0345\tPASS\t.\t.\t.\n\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2106)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:2076)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor_region3_18(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6326split_Block(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89)\n\tat is.hail.backend.spark.SparkBackend$$anon$4.compute(SparkBackend.scala:439)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: is.hail.utils.HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:215)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3(LoadVCF.scala:521)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3$adapted(LoadVCF.scala:521)\n\tat scala.Option.foreach(Option.scala:407)\n\tat is.hail.io.vcf.VCFLine.parseAddVariant(LoadVCF.scala:521)\n\tat is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1483)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2092)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4(SparkBackend.scala:450)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4$adapted(SparkBackend.scala:449)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:449)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:85)\n\tat __C6303Compile.apply(Emit.scala)\n\tat is.hail.expr.ir.LoweredTableReader$.$anonfun$makeCoercer$15(TableIR.scala:403)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$2(ExecuteContext.scala:162)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:161)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:160)\n\tat is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:402)\n\tat is.hail.expr.ir.GenericTableValue.getLTVCoercer(GenericTableValue.scala:188)\n\tat is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:221)\n\tat is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:2140)\n\tat is.hail.expr.ir.TableReader.lower(TableIR.scala:611)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1063)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2113)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1252)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:729)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1022)\n\tat is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27)\n\tat is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11)\n\tat is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:94)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:28)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:60)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:65)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:86)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:37)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:37)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:34)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:81)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2(LoweringPipeline.scala:22)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2$adapted(LoweringPipeline.scala:20)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:20)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:11)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:48)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:550)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:539)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$4(BackendServer.scala:93)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.backend.ExecuteContext.local(ExecuteContext.scala:220)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$3(BackendServer.scala:91)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:15)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$2(BackendServer.scala:90)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$2$adapted(BackendServer.scala:89)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:100)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:100)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:83)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:82)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:406)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:15)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:22)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:387)\n\tat is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:159)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:408)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nis.hail.utils.HailException: file:/home/julia/Documents/thesis/vcf_for_generate/000070000080.vcf.gz:offset 5026524526671: error while parsing line\nHPV61\t5833\t.\tA\tC\t88.0345\tPASS\t.\t.\t.\n\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2106)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:2076)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor_region3_18(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6326split_Block(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89)\n\tat is.hail.backend.spark.SparkBackend$$anon$4.compute(SparkBackend.scala:439)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nis.hail.utils.HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:215)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3(LoadVCF.scala:521)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3$adapted(LoadVCF.scala:521)\n\tat scala.Option.foreach(Option.scala:407)\n\tat is.hail.io.vcf.VCFLine.parseAddVariant(LoadVCF.scala:521)\n\tat is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1483)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2092)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:2076)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor_region3_18(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6326split_Block(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89)\n\tat is.hail.backend.spark.SparkBackend$$anon$4.compute(SparkBackend.scala:439)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\n\n\n\n\nHail version: 0.2.134-952ae203dbbe\nError summary: HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFatalError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed eval>:1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-1477>:2\u001b[39m, in \u001b[36mwrite\u001b[39m\u001b[34m(self, output, overwrite, stage_locally, _codec_spec, _partitions)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/typecheck/check.py:585\u001b[39m, in \u001b[36m_make_dec.<locals>.wrapper\u001b[39m\u001b[34m(__original_func, *args, **kwargs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(__original_func: Callable[..., T], *args, **kwargs) -> T:\n\u001b[32m    584\u001b[39m     args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method)\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/matrixtable.py:2810\u001b[39m, in \u001b[36mMatrixTable.write\u001b[39m\u001b[34m(self, output, overwrite, stage_locally, _codec_spec, _partitions)\u001b[39m\n\u001b[32m   2807\u001b[39m     _partitions_type = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2809\u001b[39m writer = ir.MatrixNativeWriter(output, overwrite, stage_locally, _codec_spec, _partitions, _partitions_type)\n\u001b[32m-> \u001b[39m\u001b[32m2810\u001b[39m \u001b[43mEnv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMatrixWrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/backend/spark_backend.py:217\u001b[39m, in \u001b[36mSparkBackend.execute\u001b[39m\u001b[34m(self, ir, timed)\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fatal:\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfatal\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/backend/spark_backend.py:209\u001b[39m, in \u001b[36mSparkBackend.execute\u001b[39m\u001b[34m(self, ir, timed)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, ir: BaseIR, timed: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Any:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copy_log_on_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/backend/backend.py:181\u001b[39m, in \u001b[36mBackend.execute\u001b[39m\u001b[34m(self, ir, timed)\u001b[39m\n\u001b[32m    179\u001b[39m     result, timings = \u001b[38;5;28mself\u001b[39m._rpc(ActionTag.EXECUTE, payload)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.maybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ir.typ == tvoid:\n\u001b[32m    183\u001b[39m     value = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/backend/backend.py:179\u001b[39m, in \u001b[36mBackend.execute\u001b[39m\u001b[34m(self, ir, timed)\u001b[39m\n\u001b[32m    177\u001b[39m payload = ExecutePayload(\u001b[38;5;28mself\u001b[39m._render_ir(ir), \u001b[33m'\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStreamBufferSpec\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m}\u001b[39m\u001b[33m'\u001b[39m, timed)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     result, timings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mActionTag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEXECUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FatalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.maybe_user_error(ir) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/hail/lib/python3.12/site-packages/hail/backend/py4j_backend.py:232\u001b[39m, in \u001b[36mPy4JBackend._rpc\u001b[39m\u001b[34m(self, action, payload)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code >= \u001b[32m400\u001b[39m:\n\u001b[32m    231\u001b[39m     error_json = orjson.loads(resp.content)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m fatal_error_from_java_error_triplet(\n\u001b[32m    233\u001b[39m         error_json[\u001b[33m'\u001b[39m\u001b[33mshort\u001b[39m\u001b[33m'\u001b[39m], error_json[\u001b[33m'\u001b[39m\u001b[33mexpanded\u001b[39m\u001b[33m'\u001b[39m], error_json[\u001b[33m'\u001b[39m\u001b[33merror_id\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp.content, parse_timings(resp.headers.get(\u001b[33m'\u001b[39m\u001b[33mX-Hail-Timings\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[31mFatalError\u001b[39m: HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25) (192.168.0.112 executor driver): is.hail.utils.HailException: file:/home/julia/Documents/thesis/vcf_for_generate/000070000080.vcf.gz:offset 5026524526671: error while parsing line\nHPV61\t5833\t.\tA\tC\t88.0345\tPASS\t.\t.\t.\n\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2106)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:2076)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor_region3_18(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6326split_Block(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89)\n\tat is.hail.backend.spark.SparkBackend$$anon$4.compute(SparkBackend.scala:439)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: is.hail.utils.HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:215)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3(LoadVCF.scala:521)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3$adapted(LoadVCF.scala:521)\n\tat scala.Option.foreach(Option.scala:407)\n\tat is.hail.io.vcf.VCFLine.parseAddVariant(LoadVCF.scala:521)\n\tat is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1483)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2092)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4(SparkBackend.scala:450)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$parallelizeAndComputeWithIndex$4$adapted(SparkBackend.scala:449)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:449)\n\tat is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:85)\n\tat __C6303Compile.apply(Emit.scala)\n\tat is.hail.expr.ir.LoweredTableReader$.$anonfun$makeCoercer$15(TableIR.scala:403)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$2(ExecuteContext.scala:162)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:161)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:160)\n\tat is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:402)\n\tat is.hail.expr.ir.GenericTableValue.getLTVCoercer(GenericTableValue.scala:188)\n\tat is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:221)\n\tat is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:2140)\n\tat is.hail.expr.ir.TableReader.lower(TableIR.scala:611)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1063)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2113)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1236)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2112)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1197)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1252)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1052)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1656)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:729)\n\tat is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1022)\n\tat is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27)\n\tat is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11)\n\tat is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:94)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:28)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:60)\n\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:65)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:86)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:37)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:37)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:34)\n\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:81)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2(LoweringPipeline.scala:22)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$2$adapted(LoweringPipeline.scala:20)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:20)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:11)\n\tat is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:48)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$execute$1(SparkBackend.scala:550)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:98)\n\tat is.hail.backend.ExecuteContext.time(ExecuteContext.scala:183)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:539)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$4(BackendServer.scala:93)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.backend.ExecuteContext.local(ExecuteContext.scala:220)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$3(BackendServer.scala:91)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:15)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$2(BackendServer.scala:90)\n\tat is.hail.backend.BackendHttpHandler.$anonfun$handle$2$adapted(BackendServer.scala:89)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:100)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:100)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:83)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13)\n\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:82)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:406)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:15)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:22)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:387)\n\tat is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692)\n\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:159)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442)\n\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:408)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nis.hail.utils.HailException: file:/home/julia/Documents/thesis/vcf_for_generate/000070000080.vcf.gz:offset 5026524526671: error while parsing line\nHPV61\t5833\t.\tA\tC\t88.0345\tPASS\t.\t.\t.\n\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2106)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:2076)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor_region3_18(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6326split_Block(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89)\n\tat is.hail.backend.spark.SparkBackend$$anon$4.compute(SparkBackend.scala:439)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nis.hail.utils.HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'.\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19)\n\tat is.hail.utils.package$.fatal(package.scala:96)\n\tat is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:215)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3(LoadVCF.scala:521)\n\tat is.hail.io.vcf.VCFLine.$anonfun$parseAddVariant$3$adapted(LoadVCF.scala:521)\n\tat scala.Option.foreach(Option.scala:407)\n\tat is.hail.io.vcf.VCFLine.parseAddVariant(LoadVCF.scala:521)\n\tat is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1483)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:2092)\n\tat is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:2076)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor_region3_18(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6396split_StreamFor(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.__m6326split_Block(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat __C6314collect_distributed_array_table_coerce_sortedness.apply(Unknown Source)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90)\n\tat is.hail.utils.package$.using(package.scala:673)\n\tat is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166)\n\tat is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89)\n\tat is.hail.backend.spark.SparkBackend$$anon$4.compute(SparkBackend.scala:439)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\n\n\n\n\nHail version: 0.2.134-952ae203dbbe\nError summary: HailException: Invalid locus 'HPV61:5833' found. Contig 'HPV61' is not in the reference genome 'GRCh38'."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "af.write(AF_PATH, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df4b0fbb-b517-4e81-a529-8ef87ab5c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'is_female': bool\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'allele_frequencies': array<float64>\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'DP': int32\n",
      "    'AD': array<int32>\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "af.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c83a3f20-0630-4c20-9ccc-d278754d7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10918244, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b2afb-882b-4753-96d1-5ae63322001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_af = combined_mt.filter_rows(\n",
    "    (combined_mt.locus.contig == \"22\") & \n",
    "    (combined_mt.locus.position == 50808270)\n",
    ")\n",
    "\n",
    "final_af.rows().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62989db6-613a-4ee0-bf8f-8de213632a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"RemoteBlock-temp-file-clean-thread\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager$$Lambda$985/0x0000000100489040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager.org$apache$spark$storage$BlockManager$RemoteBlockDownloadFileManager$$keepCleaning(BlockManager.scala:2228)\n",
      "\tat org.apache.spark.storage.BlockManager$RemoteBlockDownloadFileManager$$anon$2.run(BlockManager.scala:2194)\n"
     ]
    }
   ],
   "source": [
    "hl.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
